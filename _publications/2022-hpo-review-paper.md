---
title: "A survey on multi-objective hyperparameter optimization algorithms for machine learning"
collection: publications
category: manuscripts
permalink: /publication/2022-hpo-review-paper
excerpt: 'This article systematically surveys multi-objective hyperparameter optimization methods from 2014–2020, categorizing algorithms, evaluating comparison metrics, and highlighting future research directions.'
date: 2022-12-24
venue: 'Artificial Intelligence Review'
paperurl: 'https://doi.org/10.1007/s10462-022-10359-2'
bibtexurl: '../files/hpo-review_bibtex.bib'
citation: 'Morales-Hernández, A., Van Nieuwenhuyse, I. & Rojas Gonzalez, S. (2022). &quot;A survey on multi-objective hyperparameter optimization algorithms for machine learning.&quot; <i>Artificial Intelligence Review</i>. 56.'
---

Hyperparameter optimization (HPO) is a necessary step to ensure the best possible performance of Machine Learning (ML) algorithms. Several methods have been developed to perform HPO; most of these are focused on optimizing one performance measure (usually an error-based measure), and the literature on such single-objective HPO problems is vast. Recently, though, algorithms have appeared that focus on optimizing multiple conflicting objectives simultaneously. This article presents a systematic survey of the literature published between 2014 and 2020 on multi-objective HPO algorithms, distinguishing between metaheuristic-based algorithms, metamodel-based algorithms and approaches using a mixture of both. We also discuss the quality metrics used to compare multi-objective HPO procedures and present future research directions.
